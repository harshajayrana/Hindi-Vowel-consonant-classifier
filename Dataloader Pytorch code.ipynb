{"cells":[{"metadata":{"id":"PeozigfFQ850","trusted":true},"cell_type":"code","source":"import os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset, random_split\nimport torchvision.transforms as transforms\n\n#For converting the dataset to torchvision dataset format\nclass VowelConsonantDataset(Dataset):\n    def __init__(self, file_path,train=True,transform=None):\n        self.transform = transform\n        self.file_path=file_path\n        self.train=train\n        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n        self.len = len(self.file_names)\n        if self.train:\n            self.classes_mapping=self.get_classes()\n    def __len__(self):\n        return len(self.file_names)\n    \n    def __getitem__(self, index):\n        file_name=self.file_names[index]\n        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n        if self.transform:\n            image_data = self.transform(image_data)\n        if self.train:\n            file_name_splitted=file_name.split(\"_\")\n            Y1 = self.classes_mapping[file_name_splitted[0]]\n            Y2 = self.classes_mapping[file_name_splitted[1]]\n            z1,z2=torch.zeros(10),torch.zeros(10)\n            z1[Y1-10],z2[Y2]=1,1\n            label=torch.stack([z1,z2])\n\n            return image_data, label\n\n        else:\n            return image_data, file_name\n          \n    def pil_loader(self,path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')\n\n      \n    def get_classes(self):\n        classes=[]\n        for name in self.file_names:\n            name_splitted=name.split(\"_\")\n            classes.extend([name_splitted[0],name_splitted[1]])\n        classes=list(set(classes))\n        classes_mapping={}\n        for i,cl in enumerate(sorted(classes)):\n            classes_mapping[cl]=i\n        return classes_mapping\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"rJJ_MrEVQ_eX","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\n\nimport torchvision.transforms as transforms\n\nimport numpy as np\nimport pandas as pd\n\ntrain_on_gpu = torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"id":"S-BX7SIgS_bU","trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])","execution_count":null,"outputs":[]},{"metadata":{"id":"X2cOje6hS_yA","trusted":true},"cell_type":"code","source":"full_data = VowelConsonantDataset(\"../input/padhai-hindi-vow-cons-classification/train/train\",train=True,transform=transform)\ntrain_size = int(0.9 * len(full_data))\ntest_size = len(full_data) - train_size\n\ntrain_data, validation_data = random_split(full_data, [train_size, test_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_data, batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Uzv8q9j63A_L","trusted":true},"cell_type":"code","source":"test_data = VowelConsonantDataset(\"../input/padhai-hindi-vow-cons-classification/test/test\",train=False,transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=32,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Kaggle(nn.Module):\n  def __init__(self):\n    super(Kaggle, self).__init__()\n    self.model_resnet = models.resnet50(pretrained = True, progress = True)\n    final_in_features = self.model_resnet.fc.in_features\n    self.model_resnet.fc = nn.Linear(final_in_features, 2048)\n#    self.cnn_model = nn.Sequential(\n#        nn.Conv2d(3, 6, 3, stride = 1, padding = 1),\n#        nn.ReLU(),\n#        nn.AvgPool2d(2, stride = 2),\n#        nn.Conv2d(6, 16, 3, stride = 1, padding = 1),\n#        nn.ReLU(),\n#        nn.AvgPool2d(2, stride = 2),\n#    )\n#    self.cnn_model2 = nn.Sequential(\n#        nn.Conv2d(3, 6, 3, stride = 1, padding = 1),\n#        nn.ReLU(),\n#        nn.AvgPool2d(2, stride = 2),\n#        nn.Conv2d(6, 16, 3, stride = 1, padding = 1),\n#        nn.ReLU(),\n#        nn.AvgPool2d(2, stride = 2),\n#    )\n    self.fc_model = nn.Sequential(\n        nn.BatchNorm1d(2048),\n        nn.Dropout(0.3),\n        nn.Linear(2048, 256),\n        nn.ReLU(),\n        nn.Linear(256, 10),\n    )\n    self.fc_model2 = nn.Sequential(\n        nn.BatchNorm1d(2048),\n        nn.Dropout(0.3),\n        nn.Linear(2048, 256),\n        nn.ReLU(),\n        nn.Linear(256, 10),  \n    )\n\n  def forward (self, x):\n    x = self.model_resnet(x)\n    x = x.view(x.size(0), -1) # compressing into ( batch, the rest terms )\n    x1 = self.fc_model(x)\n    x2 = self.fc_model2(x)\n    return x1, x2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Kaggle())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kag = Kaggle()\nkag = kag.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluation(dataloader, model):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        output1, output2 = kag.forward(inputs)\n        _, pred1 = torch.max(output1.data, 1)\n        _, pred2 = torch.max(output2.data, 1)\n        _,labels1=torch.max(labels[:,0,:].data,1)\n        _,labels2=torch.max(labels[:,1,:].data,1)\n        total += labels.size(0)\n        corr1 = (pred1==labels1)\n        corr2 = (pred2==labels2)\n        correct += (corr1 == corr2).sum().item()\n    return 100 * correct / total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(kag.parameters(), lr = 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nloss_arr = []\nloss_epoch_arr = []\nmax_epochs = 1\n\nfor epochs in range(max_epochs):\n  for i, data in enumerate(train_loader, 0):\n    kag.train()\n    images, labels = data\n    images = images.to(device)\n    \n    #print(labels.shape)\n    labels1 = labels[:, 0, :].cpu()\n    labels2 = labels[:, 1, :].cpu()\n    #print(labels1.shape)\n    labels1 = np.argmax(labels1, axis = 1)\n    #print(labels1.shape)\n    labels2 = np.argmax(labels2, axis = 1)\n    labels1 = labels1.to(device)\n    labels2 = labels2.to(device)\n    #print(labels1, labels2)\n    \n    opt.zero_grad()    \n    output1, output2 = kag.forward(images)\n    #print(output1.shape)\n    #print(labels1.shape)\n\n    loss1 = loss_fn(output1, labels1)\n    loss2 = loss_fn(output2, labels2)\n    \n    loss = torch.add(loss1, loss2)\n    loss.backward()\n    opt.step()\n    \n    loss_arr.append(loss.item())\n  loss_epoch_arr.append(loss.item())\n  print(\"Train Accuracy :\",evaluation(train_loader,kag))\nplt.plot(loss_arr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluation(validation_loader,kag))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for images, labels in test_loader:\n#    print(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kag.eval()\nprediction = []\nactual_label = []\nfor inputs, label in test_loader:\n    inputs = inputs.to(device)\n    outputs = kag.model_resnet(inputs)\n    print(outputs.shape)\n    out1 = kag.fc_model(outputs)\n    out2 = kag.fc_model2(outputs)\n    _,pred1=torch.max(out1,1)\n    pred1=pred1.tolist()\n    _,pred2=torch.max(out2,1)\n    pred2=pred2.tolist()\n    for x,y,z in zip(pred1,pred2,label):\n        pred = \"V\"+str(x)+\"_\"+\"C\"+str(y)\n        print(pred)\n        prediction.append(pred)\n        actual_label.append(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"ImageId\": actual_label, \"Class\": prediction})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}